{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee7026d-6d90-4567-9fcf-1a711cb1e77f",
   "metadata": {},
   "source": [
    "Grid Search K-Fold Cross Validation to properly find best performing hyperparameters on the dataset and to calculate robust performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951bf687-4e6f-4009-ab9f-1bd346265b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold\n",
    "%run Full_Neural_Network.ipynb\n",
    "\n",
    "#load in the dataset and shuffle\n",
    "df = pd.read_csv(\"heart_failure_clinical_records_dataset.csv\")\n",
    "data = df.to_numpy()\n",
    "np.random.shuffle(data)\n",
    "\n",
    "def runPipeline(data):\n",
    "    #set up hyperparameters and parse the data properly\n",
    "    params = {\"layer_widths\": [24, 36, 48], \"learning_rates\": [ .0012, .001, .00098, .00096], \n",
    "              \"l2\" : [.0009, .001, .003], \"epochs\" : [10000, 25000, 50000]}\n",
    "    labels = data[:, -1]\n",
    "    labels = labels.astype(int)\n",
    "    features = data[:, :-1]\n",
    "    res = runTuneTest(labels, features, params)\n",
    "    print(\"Articial Neural Network Grid Search CV:\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    for i in res:\n",
    "        print(\"Parameters:\")\n",
    "        print(i[1])\n",
    "        print(\"Accuracy:\")\n",
    "        print(i[0])\n",
    "        \n",
    "        \n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        \n",
    "def runTuneTest(labels, features, params):\n",
    "    #perform a 2-Fold Cross Validation and pass the training set to GridSearch\n",
    "    res = []\n",
    "    skf = StratifiedKFold(n_splits=2)\n",
    "    splits = skf.split(X=data, y=labels)\n",
    "    for i, (train_index, test_index) in enumerate(splits):\n",
    "        trainFeatures = [features[index] for index in train_index]\n",
    "        trainLabels = [labels[index] for index in train_index]\n",
    "        testFeatures = [features[index] for index in test_index]\n",
    "        testLabels = [labels[index] for index in test_index]\n",
    "        trainFeatures = np.array(trainFeatures)\n",
    "        trainLabels = np.array(trainLabels)\n",
    "        testFeatures = np.array(testFeatures)\n",
    "        testLabels = np.array(testLabels)\n",
    "        clf = GridSearch(trainFeatures, trainLabels, params)\n",
    "        acc = trainANN(clf[\"learning_rates\"], clf[\"layer_widths\"], clf[\"l2\"], clf[\"epochs\"], \n",
    "                        trainFeatures, trainLabels, testFeatures, testLabels)\n",
    "        res.append([acc, clf])\n",
    "    return res\n",
    "        \n",
    "def GridSearch(trainFeatures, trainLabels, params):\n",
    "    #Perform another 2-Fold Cross Validation and find best hyperparameters \n",
    "    #for the ANN\n",
    "    skf = StratifiedKFold(n_splits=2)\n",
    "    splits = skf.split(X=trainFeatures, y=trainLabels)\n",
    "    best_params = {\"layer_widths\": 0, \"learning_rates\": 0, \"epochs\": 0, \"l2\": 0}\n",
    "    best_acc = 0\n",
    "    for i, (train_index, test_index) in enumerate(splits):\n",
    "        gdTrain_Features = [trainFeatures[index] for index in train_index]\n",
    "        gdTrain_Labels = [trainLabels[index] for index in train_index]\n",
    "        gdTest_Features = [trainFeatures[index] for index in test_index]\n",
    "        gdTest_Labels = [trainLabels[index] for index in test_index]\n",
    "            \n",
    "        gdTrain_Features = np.array(gdTrain_Features)\n",
    "        gdTrain_Labels = np.array(gdTrain_Labels)\n",
    "        gdTest_Features = np.array(gdTest_Features)\n",
    "        gdTest_Labels = np.array(gdTest_Labels)\n",
    "        \n",
    "        for width in params[\"layer_widths\"]:\n",
    "            for learning_rate in params[\"learning_rates\"]:\n",
    "                for epoch in params[\"epochs\"]:\n",
    "                    for l2 in params[\"l2\"]:\n",
    "                        acc = trainANN(learning_rate, width, l2, epoch, \n",
    "                                gdTrain_Features, gdTrain_Labels, gdTest_Features, gdTest_Labels)\n",
    "                        if acc>best_acc:\n",
    "                            best_acc= acc\n",
    "                            best_params[\"layer_widths\"] = width\n",
    "                            best_params[\"learning_rates\"] = learning_rate\n",
    "                            best_params[\"epochs\"] = epoch\n",
    "                            best_params[\"l2\"] = l2\n",
    "        \n",
    "    #return the best performing parameters\n",
    "    return best_params\n",
    "                \n",
    "\n",
    "                \n",
    "def trainANN(lr, width, l2_regularizer, epochs, trFeatures, trLabels, teFeatures, teLabels):\n",
    "    #set up the layers and the hyperparameters for the ANN\n",
    "    dense1 = Layer_Dense(12, width, weight_regularizer_l2 = l2_regularizer,\n",
    "        bias_regularizer_l2 = l2_regularizer)\n",
    "    activation1 = Activation_ReLU()\n",
    "    dense2 = Layer_Dense(width, width)\n",
    "    activation2 = Activation_ReLU()\n",
    "    dense3 = Layer_Dense(width, 2)\n",
    "    loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "    optimizer = Optimizer_Adam(learning_rate= lr, decay=5e-4)\n",
    "    #training block\n",
    "    for epoch in range(epochs):\n",
    "        # Perform a forward pass of our training data through all layers and activation functions\n",
    "        dense1.forward(trFeatures)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "        dense3.forward(activation2.output)\n",
    "        #loss given our softmax output\n",
    "        data_loss = loss_activation.forward(dense3.output, trLabels)\n",
    "        #L2 loss\n",
    "        regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "        #total loss\n",
    "        loss = data_loss + regularization_loss\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        accuracy = np.mean(predictions==trLabels)\n",
    "        #backpropogate\n",
    "        loss_activation.backward(loss_activation.output, trLabels)\n",
    "        dense3.backward(loss_activation.dinputs)\n",
    "        activation2.backward(dense3.dinputs)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        # Update weights and biases\n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        optimizer.post_update_params()\n",
    "    #test testing data\n",
    "    dense1.forward(teFeatures)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    dense3.forward(activation2.output)\n",
    "    loss = loss_activation.forward(dense3.output, teLabels)\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    accuracy = np.mean(predictions == teLabels)\n",
    "    return accuracy\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "269bb0e3-5026-4fba-b9b6-382d4561a961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articial Neural Network Grid Search CV:\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Parameters:\n",
      "{'layer_widths': 48, 'learning_rates': 0.0012, 'epochs': 10000, 'l2': 0.0009}\n",
      "Accuracy:\n",
      "0.7866666666666666\n",
      "Parameters:\n",
      "{'layer_widths': 24, 'learning_rates': 0.0012, 'epochs': 50000, 'l2': 0.0009}\n",
      "Accuracy:\n",
      "0.8053691275167785\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "runPipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eefdf0-8867-4c27-b824-31b9031de5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "runPipeline(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
